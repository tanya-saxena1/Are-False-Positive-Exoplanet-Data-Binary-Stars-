{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeaaWxhZYs-x",
        "outputId": "30e6a91d-361c-449d-a0bb-e30360e298fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-a698d1c6b9f6>:6: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import gaussian_filter1d, uniform_filter1d, median_filter\n",
            "<ipython-input-1-a698d1c6b9f6>:6: DeprecationWarning: Please use `uniform_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import gaussian_filter1d, uniform_filter1d, median_filter\n",
            "<ipython-input-1-a698d1c6b9f6>:6: DeprecationWarning: Please use `median_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import gaussian_filter1d, uniform_filter1d, median_filter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries Loaded!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, f1_score\n",
        "from scipy.ndimage.filters import gaussian_filter1d, uniform_filter1d, median_filter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "#Import libraries for Deep Learning\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv1D, LSTM, MaxPool1D, Dense, Dropout, Flatten, BatchNormalization, Input, concatenate, Activation\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "print ('Libraries Loaded!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fourier(data):\n",
        "    datafft= np.zeros(data.shape)\n",
        "    for row in range(data.shape[0]):\n",
        "        #fft\n",
        "        fou=np.fft.fft(data[row])\n",
        "        f1,f2= 0, 0\n",
        "        fr=np.fft.fftfreq(3197,0.01)\n",
        "        #make up a narrow bandpass with a Gaussian\n",
        "        df=0.1\n",
        "        gpl= np.exp(- ((fr-f1)/(2*df))**2)+ np.exp(- ((fr-f2)/(2*df))**2)  # pos. frequencies\n",
        "        gmn= np.exp(- ((fr+f1)/(2*df))**2)+ np.exp(- ((fr+f2)/(2*df))**2)  # neg. frequencies\n",
        "        g=gpl+gmn\n",
        "        filt=fou*g  #filtered spectrum = spectrum * bandpass\n",
        "        #ifft\n",
        "        s2=np.fft.ifft(filt)\n",
        "        datafft[row]= s2\n",
        "    return (datafft)"
      ],
      "metadata": {
        "id": "_qz2Eovwcut_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('ExoTrain.csv')\n",
        "Y1 = data1['LABEL'].values - 1\n",
        "X = data1.drop('LABEL', axis=1).values\n",
        "Y = Y1[:,np.newaxis]"
      ],
      "metadata": {
        "id": "CBMFYFa8dA6i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "f455f209-5aaa-4a28-ffff-eb84d5571479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'ExoTrain.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-35b1a463cd1b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ExoTrain.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LABEL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LABEL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ExoTrain.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "A4HQllFuWy4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label= 1             # Here label i is exoplanet and 0 is non-planet\n",
        "num_row=10           # Number of rows to plot\n",
        "col_plot=5           # 5 plots in a row\n",
        "j=1\n",
        "train= data1          # plots from training data\n",
        "\n",
        "print ('Label=', label,'\\n\\\n",
        "First Column-      Original\\n\\\n",
        "Second Column-     Rolling Mean\\n\\\n",
        "Third Column-      Median Filter\\n\\\n",
        "Fourth Column-     Uniform Filter\\n\\\n",
        "Fifth Column-      Fourier Filter ')\n",
        "fig = plt.figure(figsize=(8*col_plot, num_row*5))\n",
        "for i in range(num_row):\n",
        "\n",
        "    fig.add_subplot(num_row,col_plot,j); j=j+1\n",
        "    new1= train[train.LABEL==label].iloc[i, :].drop('LABEL')\n",
        "    new1.plot(color= 'red', label= 'new1')\n",
        "\n",
        "    fig.add_subplot(num_row,col_plot,j); j=j+1\n",
        "    new2 = train[train.LABEL==label].iloc[i, :].drop('LABEL').rolling(window=100).mean()\n",
        "    new2.plot(color= 'blue', label= 'new2')\n",
        "\n",
        "    fig.add_subplot(num_row,col_plot,j); j=j+1\n",
        "    new3=median_filter(train[train.LABEL==label].iloc[i, :].drop('LABEL'), size=100)\n",
        "    plt.plot(new3, color= 'black', label= 'new3')\n",
        "\n",
        "    fig.add_subplot(num_row,col_plot,j); j=j+1\n",
        "    new4= uniform_filter1d(train[train.LABEL==label].iloc[i, :].drop('LABEL'), size=100)\n",
        "    plt.plot(new4, color= 'black', label= 'new4')\n",
        "\n",
        "    fig.add_subplot(num_row,col_plot,j); j=j+1\n",
        "    new5= fourier(train[train.LABEL==label].iloc[i, :].drop('LABEL').values[np.newaxis, :])\n",
        "    plt.plot(new5[0], color= 'black', label= 'new5')"
      ],
      "metadata": {
        "id": "ADnalTfqdJZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=531, stratify=Y)"
      ],
      "metadata": {
        "id": "EJL5p3XqmwZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scale each observation to zero mean and unit variance.\n",
        "x_train = ((x_train - np.mean(x_train, axis=1).reshape(-1,1)) /\n",
        "           np.std(x_train, axis=1).reshape(-1,1))\n",
        "x_test = ((x_test - np.mean(x_test, axis=1).reshape(-1,1)) /\n",
        "          np.std(x_test, axis=1).reshape(-1,1))"
      ],
      "metadata": {
        "id": "uSHDoDEbm2d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Noise in the data can be ignored by adding a gaussian filter\n",
        "#This could more elegantly be done as an extra layer with fixed weights.\n",
        "x_train1 = np.stack([x_train, gaussian_filter1d(x_train, 1, axis=1)], axis=2)\n",
        "x_test1 = np.stack([x_test, gaussian_filter1d(x_test, 1, axis=1)], axis=2)"
      ],
      "metadata": {
        "id": "-0pMcjzxm45z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train2 = np.stack([x_train, fourier(x_train)], axis=2)\n",
        "x_test2 = np.stack([x_test, fourier(x_test)], axis=2)"
      ],
      "metadata": {
        "id": "iGpv-5d1m84Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train3 = x_train[:,:,np.newaxis]\n",
        "x_test3 = x_test[:,:,np.newaxis]"
      ],
      "metadata": {
        "id": "k3ZKHZRNnBcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Shape of x_train1', x_train1.shape)\n",
        "print ('Shape of x_test1', x_test1.shape)\n",
        "print(\"*\"*40)\n",
        "print ('Shape of x_train2', x_train2.shape)\n",
        "print ('Shape of x_test2', x_test2.shape)\n",
        "print(\"*\"*40)\n",
        "print ('Shape of x_train3', x_train3.shape)\n",
        "print ('Shape of x_test3', x_test3.shape)"
      ],
      "metadata": {
        "id": "vLC5VG1AnEWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-D convolutions are frequently used to learn sequential patterns\n",
        "model1 = Sequential()\n",
        "model1.add(Conv1D(filters=16, kernel_size=11, activation='relu', input_shape=x_train1.shape[1:]))\n",
        "model1.add(MaxPool1D(strides=4))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Conv1D(filters=64, kernel_size=11, activation='relu'))\n",
        "model1.add(MaxPool1D(strides=4))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Conv1D(filters=32, kernel_size=11, activation='relu'))\n",
        "model1.add(MaxPool1D(strides=4))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Conv1D(filters=32, kernel_size=11, activation='relu'))\n",
        "model1.add(MaxPool1D(strides=4))\n",
        "model1.add(Flatten())\n",
        "model1.add(Dropout(0.3))\n",
        "model1.add(Dense(32, activation='relu'))\n",
        "model1.add(Dropout(0.3))\n",
        "model1.add(Dense(64, activation='relu'))\n",
        "model1.add(Dropout(0.3))\n",
        "model1.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "dEIfC1JhnKOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-D convolutions are frequently used to learn sequential patterns\n",
        "model2 = Sequential()\n",
        "model2.add(Conv1D(filters=24, kernel_size=11, activation='relu', input_shape=x_train2.shape[1:]))\n",
        "model2.add(MaxPool1D(strides=4))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Conv1D(filters=64, kernel_size=7, activation='relu'))\n",
        "model2.add(MaxPool1D(strides=4))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Conv1D(filters=64, kernel_size=11, activation='relu'))\n",
        "model2.add(MaxPool1D(strides=4))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Conv1D(filters=64, kernel_size=11, activation='relu'))\n",
        "model2.add(MaxPool1D(strides=4))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dropout(0.25))\n",
        "model2.add(Dense(32, activation='relu'))\n",
        "model2.add(Dropout(0.25))\n",
        "model2.add(Dense(64, activation='relu'))\n",
        "model2.add(Dropout(0.25))\n",
        "model2.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "6NG_h0TMnPGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = Sequential()\n",
        "model3.add(Conv1D(filters=16, kernel_size=11, activation='relu', input_shape=x_train3.shape[1:]))\n",
        "model3.add(MaxPool1D(strides=4))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Conv1D(filters=64, kernel_size=11, activation='relu'))\n",
        "model3.add(MaxPool1D(strides=4))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Conv1D(filters=32, kernel_size=11, activation='relu'))\n",
        "model3.add(MaxPool1D(strides=4))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Conv1D(filters=32, kernel_size=11, activation='relu'))\n",
        "model3.add(MaxPool1D(strides=4))\n",
        "model3.add(LSTM(10))\n",
        "model3.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "AdyVoBbLnT-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_input= Input(shape= x_train1.shape[1:])\n",
        "#x= Conv1D(filters=1, kernel_size=11, activation='relu', padding='same')(x_input)\n",
        "#x=BatchNormalization()(x)\n",
        "\n",
        "#from keras.layers import concatenate\n",
        "\n",
        "#m = concatenate([x, x_input])\n",
        "\n",
        "#x= Conv1D(filters=16, kernel_size=11, activation='relu')(m)\n",
        "#x=MaxPool1D(strides=4)(x)\n",
        "#x=BatchNormalization()(x)\n",
        "\n",
        "#x= Conv1D(filters=16, kernel_size=11, activation='relu')(x)\n",
        "#x=MaxPool1D(strides=4)(x)\n",
        "#x=BatchNormalization()(x)\n",
        "\n",
        "#x= Conv1D(filters=64, kernel_size=11, activation='relu')(x)\n",
        "#x= MaxPool1D(strides=4)(x)\n",
        "#x=Flatten()(x)\n",
        "#x=Dropout(.3)(x)\n",
        "#x=Dense(32, activation='relu')(x)\n",
        "#x=Dropout(0.3)(x)\n",
        "#x=Dense(64, activation='relu')(x)\n",
        "#x=Dense(1, activation='sigmoid')(x)\n",
        "#model4= Model(input=x_input, output= x)\n",
        "\n",
        "#model4.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv1D, MaxPool1D, Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import concatenate\n",
        "\n",
        "x_input = Input(shape=x_train1.shape[1:])\n",
        "x = Conv1D(filters=1, kernel_size=11, activation='relu', padding='same')(x_input)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "m = concatenate([x, x_input])\n",
        "\n",
        "x = Conv1D(filters=16, kernel_size=11, activation='relu')(m)\n",
        "x = MaxPool1D(strides=4)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Conv1D(filters=16, kernel_size=11, activation='relu')(x)\n",
        "x = MaxPool1D(strides=4)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = Conv1D(filters=64, kernel_size=11, activation='relu')(x)\n",
        "x = MaxPool1D(strides=4)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model4 = Model(inputs=x_input, outputs=x)\n",
        "\n",
        "model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "VVgmR5yYnYKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()"
      ],
      "metadata": {
        "id": "pGvHClHPon7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "id": "wCqMYFRlos7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.summary()"
      ],
      "metadata": {
        "id": "oDXzUBg8o2qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.summary()"
      ],
      "metadata": {
        "id": "WdoDuUS5o52n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feeding training data to the model in batches\n",
        "def batch_generator(x_train, y_train, batch_size=32):\n",
        "    \"\"\"\n",
        "    Gives equal number of positive and negative samples, and rotates them randomly in time\n",
        "    \"\"\"\n",
        "    half_batch = 16\n",
        "    x_batch = np.empty((batch_size, x_train.shape[1], x_train.shape[2]), dtype='float32')\n",
        "    y_batch = np.empty((batch_size, y_train.shape[1]), dtype='float32')\n",
        "\n",
        "    planet = np.where(y_train[:,0] == 1.)[0]\n",
        "    non_planet = np.where(y_train[:,0] == 0.)[0]\n",
        "\n",
        "    while True:\n",
        "        np.random.shuffle(planet)\n",
        "        np.random.shuffle(non_planet)\n",
        "\n",
        "        x_batch[:half_batch] = x_train[planet[:half_batch]]\n",
        "        x_batch[half_batch:] = x_train[non_planet[half_batch:batch_size]]\n",
        "        y_batch[:half_batch] = y_train[planet[:half_batch]]\n",
        "        y_batch[half_batch:] = y_train[non_planet[half_batch:batch_size]]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            roll_state = np.random.randint(x_batch.shape[1])\n",
        "            x_batch[i] = np.roll(x_batch[i], roll_state, axis = 0)\n",
        "\n",
        "        yield x_batch, y_batch"
      ],
      "metadata": {
        "id": "2jMgXwgVo868"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1_history = model1.fit_generator(batch_generator(x_train1, y_train, 48),\n",
        "                    validation_data=(x_test1, y_test),\n",
        "                    verbose=2, epochs=35,\n",
        "                    steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "rdj767vapAg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(optimizer=Adam(0.0001), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "model1_history2 = model1.fit_generator(batch_generator(x_train1, y_train, 48),\n",
        "                           validation_data=(x_test1, y_test),\n",
        "                           verbose=2, epochs=10,\n",
        "                           steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "qgmyv94xrsU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(optimizer=Adam(0.00001), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "model1_history3 = model1.fit_generator(batch_generator(x_train1, y_train, 48),\n",
        "                    validation_data=(x_test1, y_test),\n",
        "                    verbose=2, epochs=10,\n",
        "                    steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "MCJ57L0oshCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2_history = model2.fit_generator(batch_generator(x_train2, y_train, 48),\n",
        "                    validation_data=(x_test2, y_test),\n",
        "                    verbose=2, epochs=35,\n",
        "                    steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "UBH6Ul7XtZlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model2.compile(optimizer=Adam(0.0001), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "model2_history2 = model2.fit_generator(batch_generator(x_train2, y_train, 48),\n",
        "                           validation_data=(x_test2, y_test),\n",
        "                           verbose=2, epochs=5,\n",
        "                           steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "dSeXEy3ctZp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer=Adam(0.00001), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
        "model2_history3 = model2.fit_generator(batch_generator(x_train2, y_train, 48),\n",
        "                    validation_data=(x_test2, y_test),\n",
        "                    verbose=2, epochs=5,\n",
        "                    steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "lXn8GojfxhR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model3_history = model3.fit_generator(batch_generator(x_train3, y_train, 48),\n",
        "                    validation_data=(x_test3, y_test),\n",
        "                    verbose=2, epochs=35,\n",
        "                    steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "uAV_MruIxhjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(loss='binary_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])\n",
        "model3_history2 = model3.fit_generator(batch_generator(x_train3, y_train, 48),\n",
        "                    validation_data=(x_test3, y_test),\n",
        "                    verbose=2, epochs=10,\n",
        "                    steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "rPxcMK6YxhmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(loss='binary_crossentropy', optimizer=Adam(0.00001), metrics=['accuracy'])\n",
        "model3_history3 = model3.fit_generator(batch_generator(x_train3, y_train, 32),\n",
        "                    validation_data=(x_test3, y_test),\n",
        "                    verbose=2, epochs=5,\n",
        "                    steps_per_epoch=x_train.shape[1]//32)\n"
      ],
      "metadata": {
        "id": "GFAfew_bxhoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model4_history = model4.fit_generator(batch_generator(x_train1, y_train, 48),\n",
        "                    validation_data=(x_test1, y_test),\n",
        "                    verbose=2, epochs=35,\n",
        "                    steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "EompXKsJxhr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.compile(loss='binary_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])\n",
        "model4_history2 = model4.fit_generator(batch_generator(x_train1, y_train, 48),\n",
        "                    validation_data=(x_test1, y_test),\n",
        "                    verbose=2, epochs=10,\n",
        "                    steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "pjgKyhEMxh0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.compile(loss='binary_crossentropy', optimizer=Adam(0.00001), metrics=['accuracy'])\n",
        "model4_history3 = model4.fit_generator(batch_generator(x_train1, y_train, 48),\n",
        "                    validation_data=(x_test1, y_test),\n",
        "                    verbose=2, epochs=5,\n",
        "                    steps_per_epoch=x_train.shape[1]//32)"
      ],
      "metadata": {
        "id": "DPpVNjDi5-sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting validation loss and accuracy\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(model1_history.history['loss'], color='r', label=\"Model 1 Training loss\")\n",
        "plt.plot(model2_history.history['loss'], color='g', label=\"Model 2 Training loss\")\n",
        "plt.plot(model3_history.history['loss'], color='b', label=\"Model 3 Training loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(model1_history.history['val_loss'], color='r', label=\"Model 1 Validation loss\")\n",
        "plt.plot(model2_history.history['val_loss'], color='g', label=\"Model 2 Validation loss\")\n",
        "plt.plot(model3_history.history['val_loss'], color='b', label=\"Model 3 Validation loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(model1_history.history['accuracy'], color='r', label=\"Model 1 Training accuracy\")\n",
        "plt.plot(model2_history.history['accuracy'], color='g', label=\"Model 2 Training accuracy\")\n",
        "plt.plot(model3_history.history['accuracy'], color='b', label=\"Model 3 Training accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(model1_history.history['val_accuracy'], color='r', label=\"Model 1 Validation accuracy\")\n",
        "plt.plot(model2_history.history['val_accuracy'], color='g', label=\"Model 2 Validation accuracy\")\n",
        "plt.plot(model3_history.history['val_accuracy'], color='b', label=\"Model 3 Validation accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kU7AAdgb5-vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(model4_history.history['loss'], color='m', label=\"Training loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(model4_history.history['val_loss'], color='m', label=\"Validation loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(model4_history.history['accuracy'], color='m', label=\"Training accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(model4_history.history['val_accuracy'], color='m', label=\"Validation accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1U4gVjxD5-yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtaining the indexes of yes-exoplanet and no-exoplanet\n",
        "no_exoplanet = np.where(y_test[:,0] == 0.)[0]\n",
        "yes_exoplanet = np.where(y_test[:,0] == 1.)[0]"
      ],
      "metadata": {
        "id": "Zm-aqwPQ5-0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = model1.predict(x_test1)[:,0]\n",
        "\n",
        "y_pred2 = model2.predict(x_test2)[:,0]\n",
        "\n",
        "y_pred3 = model3.predict(x_test3)[:,0]\n",
        "\n",
        "y_pred4 = model4.predict(x_test1)[:,0]"
      ],
      "metadata": {
        "id": "xINiWilZ5-28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plots indicating the model allotting different range of scores for different classes\n",
        "# thus imploying the precision of the model\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot([y_pred1[i] for i in yes_exoplanet], 'ro', label=\"Exoplanet\")\n",
        "plt.plot([y_pred2[i] for i in yes_exoplanet], 'go', label=\"Exoplanet\")\n",
        "plt.plot([y_pred3[i] for i in yes_exoplanet], 'bo', label=\"Exoplanet\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot([y_pred1[i] for i in no_exoplanet], 'ro', label=\"Non-exoplanet\")\n",
        "plt.plot([y_pred2[i] for i in no_exoplanet], 'go', label=\"Non-exoplanet\")\n",
        "plt.plot([y_pred3[i] for i in no_exoplanet], 'bo', label=\"Non-exoplanet\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gu742CNr7V0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot([y_pred4[i] for i in yes_exoplanet], 'mo', label=\"Exoplanet\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot([y_pred4[i] for i in no_exoplanet], 'mo', label=\"Non-exoplanet\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_OPJrCug7V3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = (y_test[:, 0] + 0.5).astype(\"int\")\n",
        "f1_score_= []\n",
        "skill_score_= []\n",
        "\n",
        "for i in (np.arange(0.5,1.0,.005)):\n",
        "    y_pred_int= np.copy(y_pred1)\n",
        "    y_pred_int[(y_pred_int>i)]=1\n",
        "    y_pred_int[(y_pred_int<=i)]=0\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_int).ravel()\n",
        "\n",
        "    skill_score_.append(((tp * tn) - (fp * fn)) / ((tp + fn) * (fp + tn)))\n",
        "    f1_score_.append(f1_score(y_true, y_pred_int, average='macro'))\n",
        "\n",
        "\n",
        "\n",
        "print(\"F1 score : \", np.max(f1_score_))\n",
        "print(\"Threshold for F1 score : \", (np.min(np.where(f1_score_ == np.max(f1_score_)))+1)*0.005+0.5)\n",
        "print(\"*\"*40)\n",
        "print(\"True skill score : \", np.max(skill_score_))\n",
        "print(\"Threshold for True skill score : \", (np.min(np.where(skill_score_ == np.max(skill_score_)))+1)*0.005+0.5)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(f1_score_, color='b', label=\"F1 score\")\n",
        "plt.plot(skill_score_, color='r', label=\"True skill score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P7MCX_4C7V7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = (y_test[:, 0] + 0.5).astype(\"int\")\n",
        "f1_score_= []\n",
        "skill_score_= []\n",
        "\n",
        "for i in (np.arange(0.5,1.0,.005)):\n",
        "    y_pred_int= np.copy(y_pred2)\n",
        "    y_pred_int[(y_pred_int>i)]=1\n",
        "    y_pred_int[(y_pred_int<=i)]=0\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_int).ravel()\n",
        "\n",
        "    skill_score_.append(((tp * tn) - (fp * fn)) / ((tp + fn) * (fp + tn)))\n",
        "    f1_score_.append(f1_score(y_true, y_pred_int, average='macro'))\n",
        "\n",
        "\n",
        "\n",
        "print(\"F1 score : \", np.max(f1_score_))\n",
        "print(\"Threshold for F1 score : \", (np.min(np.where(f1_score_ == np.max(f1_score_)))+1)*0.005+0.5)\n",
        "print(\"*\"*40)\n",
        "print(\"True skill score : \", np.max(skill_score_))\n",
        "print(\"Threshold for True skill score : \", (np.min(np.where(skill_score_ == np.max(skill_score_)))+1)*0.005+0.5)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(f1_score_, color='b', label=\"F1 score\")\n",
        "plt.plot(skill_score_, color='r', label=\"True skill score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iRO9y1iP7oaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = (y_test[:, 0] + 0.5).astype(\"int\")\n",
        "f1_score_= []\n",
        "skill_score_= []\n",
        "\n",
        "for i in (np.arange(0.5,1.0,.005)):\n",
        "    y_pred_int= np.copy(y_pred3)\n",
        "    y_pred_int[(y_pred_int>i)]=1\n",
        "    y_pred_int[(y_pred_int<=i)]=0\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_int).ravel()\n",
        "\n",
        "    skill_score_.append(((tp * tn) - (fp * fn)) / ((tp + fn) * (fp + tn)))\n",
        "    f1_score_.append(f1_score(y_true, y_pred_int, average='macro'))\n",
        "\n",
        "\n",
        "\n",
        "print(\"F1 score : \", np.max(f1_score_))\n",
        "print(\"Threshold for F1 score : \", (np.min(np.where(f1_score_ == np.max(f1_score_)))+1)*0.005+0.5)\n",
        "print(\"*\"*40)\n",
        "print(\"True skill score : \", np.max(skill_score_))\n",
        "print(\"Threshold for True skill score : \", (np.min(np.where(skill_score_ == np.max(skill_score_)))+1)*0.005+0.5)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(f1_score_, color='b', label=\"F1 score\")\n",
        "plt.plot(skill_score_, color='r', label=\"True skill score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vI3eHuHx7WCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = (y_test[:, 0] + 0.5).astype(\"int\")\n",
        "f1_score_= []\n",
        "skill_score_= []\n",
        "\n",
        "for i in (np.arange(0.5,1.0,.005)):\n",
        "    y_pred_int= np.copy(y_pred4)\n",
        "    y_pred_int[(y_pred_int>i)]=1\n",
        "    y_pred_int[(y_pred_int<=i)]=0\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_int).ravel()\n",
        "\n",
        "    skill_score_.append(((tp * tn) - (fp * fn)) / ((tp + fn) * (fp + tn)))\n",
        "    f1_score_.append(f1_score(y_true, y_pred_int, average='macro'))\n",
        "\n",
        "\n",
        "\n",
        "print(\"F1 score : \", np.max(f1_score_))\n",
        "print(\"Threshold for F1 score : \", (np.min(np.where(f1_score_ == np.max(f1_score_)))+1)*0.005+0.5)\n",
        "print(\"*\"*40)\n",
        "print(\"True skill score : \", np.max(skill_score_))\n",
        "print(\"Threshold for True skill score : \", (np.min(np.where(skill_score_ == np.max(skill_score_)))+1)*0.005+0.5)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(f1_score_, color='b', label=\"F1 score\")\n",
        "plt.plot(skill_score_, color='r', label=\"True skill score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ui7OUag7y2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_pred1)):\n",
        "    if y_pred1[i]>0.97: # this value is obtained from the above plot\n",
        "        y_pred1[i] = 1\n",
        "    else:\n",
        "        y_pred1[i] = 0\n",
        "\n",
        "for i in range(len(y_pred2)):\n",
        "    if y_pred2[i]>0.505: # this value is obtained from the above plot\n",
        "        y_pred2[i] = 1\n",
        "    else:\n",
        "        y_pred2[i] = 0\n",
        "\n",
        "for i in range(len(y_pred3)):\n",
        "    if y_pred3[i]>=1: # this value is obtained from the above plot\n",
        "        y_pred3[i] = 1\n",
        "    else:\n",
        "        y_pred3[i] = 0\n",
        "\n",
        "for i in range(len(y_pred4)):\n",
        "    if y_pred4[i]>=0.97: # this value is obtained from the above plot\n",
        "        y_pred4[i] = 1\n",
        "    else:\n",
        "        y_pred4[i] = 0"
      ],
      "metadata": {
        "id": "Ceu0UKL3729F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the precision recall of each class is separately shown hence\n",
        "# removing the ambiguity of imbalanced classes..\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Model 1st report\")\n",
        "print(classification_report(y_true, y_pred1, target_names=['Explanet','Not Explanet']))\n",
        "print(\"*\"*80)\n",
        "print(\"Model 2st report\")\n",
        "print(classification_report(y_true, y_pred2, target_names=['Explanet','Not Explanet']))\n",
        "print(\"*\"*80)\n",
        "print(\"Model 3st report\")\n",
        "print(classification_report(y_true, y_pred3, target_names=['Explanet','Not Explanet']))\n",
        "print(\"*\"*80)\n",
        "print(\"Model 4st report\")\n",
        "print(classification_report(y_true, y_pred4, target_names=['Explanet','Not Explanet']))"
      ],
      "metadata": {
        "id": "yFFLc6Bb76BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/khumairraj/Exoplanet-Detection/blob/master/Code/.ipynb_checkpoints/local_test_exoplanet_detection-checkpoint.ipynb"
      ],
      "metadata": {
        "id": "oQHjD7OE-6jD"
      }
    }
  ]
}